{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recently as part of a collaboration with another lab at MSKCC, we are constructing a library of DNA oligonucleotides.  The aim of the projet itself is hush-hush, but I'd like to take a minute today and write about a less glamorous job that underpins the library construction, which is generating short random DNA sequences to identify specific groups of subsequences.  These are known as *DNA barcodes*.  \n",
    "\n",
    "In contemporary sequencing experiments, it is common for technological reasons to pool samples from different conditions or even from different experiments together as they are processed by an NGS sequencer.  DNA barcodes are used for de-multiplexing reads from these experiments post-sequencing.  It's surprising how far this technology can be pushed; a recent paper from [Cusanovich et al.](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4836442/) shows how to use combinatorial indexing with barcodes to get very cost-effective single-cell ATAC-seq.  \n",
    "\n",
    "So, people need barcodes to do sequencing.  While there does exist [software](http://comailab.genomecenter.ucdavis.edu/index.php/Barcode_generator) to [generate](https://www.nature.com/articles/s41598-017-12825-2) them, those solutions I did find were either intended to generate a very small number of codes (on the order of 10-500), or provided only codes of an unsuitable fixed length.  Some just plain did not work (**source not cited here to avoid embarrasement**).  \n",
    "\n",
    "What I needed was a barcode generator which could produce: \n",
    "- approximately 80k barcodes\n",
    "- 12 base pairs in length\n",
    "- each of which should be Hamming distance 2 or greater from the other members of the set\n",
    "- each of which should have GC content within a specified range\n",
    "- each of which should not have runs of the same base in length or three or greater\n",
    "\n",
    "This is one of those jobs which began as a chore I had to get through on the way to something more rewarding.  So I set about trying to Just Get It Over With, and wrote the simplest generator I could imagine: generate all possible 12mers, and then winnow away at subsequent chunks to find a set of barcodes that satisfies the constraints.  In the process of doing this I quickly found that my naive solution, while seemily correct, was far too slow.  So, I made some computational lemonade, and ended up exploring some fun technologies for accelerating Python code.  This post takes liberal inspiration from Jake Vanderplas' excellent [blog](https://jakevdp.github.io), where he sometimes blogs about cool Numpy tricks & moves, as well as his talk at PyCon [video](https://www.youtube.com/watch?v=EEUXKG97YRw) where he talks about exactly this type work.  Have a watch, it's time well spent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first task was to whip up a little script to generate all N-mers.  This part was easy enough:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def allstrings(alphabet, length):\n",
    "    \"\"\"Find the list of all strings of 'alphabet' of length 'length'\"\"\"\n",
    "\n",
    "    if length == 0: \n",
    "        return []\n",
    "\n",
    "    c = [[a] for a in alphabet[:]]\n",
    "    if length == 1: \n",
    "        return c\n",
    "\n",
    "    c = [[x,y] for x in alphabet for y in alphabet]\n",
    "    if length == 2: \n",
    "        return c\n",
    "\n",
    "    for l in range(2, length):\n",
    "        c = [[x]+y for x in alphabet for y in c]\n",
    "        \n",
    "    return c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "length = 12\n",
    "alphabet = ['A', 'C', 'G', 'T']\n",
    "\n",
    "my_strings = allstrings(alphabet, length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Presto, we've got a load of strings.  Here's an example from the file I generated on disk:\n",
    "```\n",
    "lski1877:barcodes zamparol$ head -n 10 all_codes.txt\n",
    "AAAAAAAAAAAA\n",
    "AAAAAAAAAAAC\n",
    "AAAAAAAAAAAG\n",
    "AAAAAAAAAAAT\n",
    "AAAAAAAAAACA\n",
    "AAAAAAAAAACC\n",
    "AAAAAAAAAACG\n",
    "AAAAAAAAAACT\n",
    "AAAAAAAAAAGA\n",
    "AAAAAAAAAAGC\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A'],\n",
       " ['A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'C'],\n",
       " ['A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'G'],\n",
       " ['A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'T'],\n",
       " ['A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'C', 'A'],\n",
       " ['A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'C', 'C'],\n",
       " ['A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'C', 'G'],\n",
       " ['A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'C', 'T'],\n",
       " ['A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'G', 'A'],\n",
       " ['A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'G', 'C']]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_strings[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's convert this list of lists into a list of strings, with a bit of code thrown in to randomize the read input.  Without this, the code spends a lot of time not doing much save for rejecting strings beginnign with `AAA..`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random.shuffle(my_strings)\n",
    "my_codes = []\n",
    "for l in my_strings:\n",
    "    my_codes.append(''.join(l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now a little code to read each of these and build a set with the desired criteria.  The first attempt is a pure Python implementation which works with strings and list comprehensions to specify the criteria for eliminating candidates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def verify_mismatch_constraints_python(barcode, barcode_set, threshold=3):\n",
    "    ''' Test if the current barcode will be at least 3 mismatches away from any \n",
    " element of the barcode set '''\n",
    "    \n",
    "    if len(barcode_set) == 0:\n",
    "        return True\n",
    "    mismatch_counts = [_mismatches(bc, barcode) for bc in barcode_set] \n",
    "    return min(mismatch_counts) >= threshold\n",
    "\n",
    "\n",
    "def _mismatches(bc, barcode):\n",
    "    counts = [1 if barcode[p] != bc[p] else 0 for p in range(len(barcode))]\n",
    "    return sum(counts)\n",
    "\n",
    "\n",
    "def verify_content_constraints_python(barcode, low = 0.10, high = 0.90):\n",
    "    ''' Make sure the GC content of the barcode lies in [low,high] \n",
    "    and also that there are no repetitive triples of any nucleotides '''\n",
    "    triples = ['AAA','TTT','GGG','CCC']\n",
    "    gc_score = [1 if b == 'C' or b == 'G' else 0 for b in barcode]\n",
    "    gc_content = sum(gc_score) / (1.0 * len(barcode))\n",
    "    \n",
    "    triple_test_list = [0 if t not in barcode else 1 for t in triples]\n",
    "    triple_test_sum = sum(triple_test_list)\n",
    "    return (low < gc_content) & (gc_content < high) & (triple_test_sum == 0)\n",
    "\n",
    "def process_batch_python(batch, barcodes):\n",
    "    ''' Apply validation rules to each potential barcode in the batch '''\n",
    "    for barcode in batch:\n",
    "        if verify_content_constraints_python(barcode) and \\\n",
    "           verify_mismatch_constraints_python(barcode, barcodes):\n",
    "            barcodes.append(barcode)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def get_codes_python(batch_size=10000, max_codes=5000):\n",
    "    ''' grow the set of barcodes one by one, processing in batches of 10000'''\n",
    "\n",
    "    batch = []\n",
    "    codes = []\n",
    "     \n",
    "    for barcode in my_codes:\n",
    "        if len(codes) >= max_codes:\n",
    "            break\n",
    "            \n",
    "        batch.append(barcode)\n",
    "        if len(batch) == batch_size:\n",
    "            t0 = time.time()\n",
    "            process_batch_python(batch, codes)\n",
    "            t1 = time.time()\n",
    "            delta_t = t1 - t0\n",
    "            batch = []\n",
    "            print(\"result of batch processing is \", len(codes), \" qualified codes\")\n",
    "            print(\"took \", delta_t)\n",
    "        \n",
    "            \n",
    "    return codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try running this a few times below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result of batch processing is  5008  qualified codes\n",
      "took  37.605226039886475\n",
      "result of batch processing is  5008  qualified codes\n",
      "took  38.993963956832886\n",
      "result of batch processing is  5008  qualified codes\n",
      "took  38.834331035614014\n",
      "result of batch processing is  5008  qualified codes\n",
      "took  44.860349893569946\n",
      "result of batch processing is  5008  qualified codes\n",
      "took  38.340277910232544\n",
      "result of batch processing is  5008  qualified codes\n",
      "took  37.76196217536926\n",
      "result of batch processing is  5008  qualified codes\n",
      "took  36.844419956207275\n",
      "result of batch processing is  5008  qualified codes\n",
      "took  37.73810791969299\n",
      "39.1 s ± 2.47 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 10000\n",
    "max_codes = 5000\n",
    "%timeit get_codes_python(batch_size, max_codes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When trying to generate the full set of barcodes, the bottleneck encountered in `verify_mismatch_constraints` was serious enough to discourage me after about 30m.  I tried to recast this as an opportunity to become more familiar with some of the technologies under the umbrella of numeric Python.  Inspired by Jake Vanderplas' talk at PyCon, my first step was to recode the strings into Numpy arrays, and to re-express the constraints as operations on the arrays accordingly.  This way, without having to come up with a better algorithm to generate compliant codes, I could just re-express my computation so that faster Numpy routines, rather than the Python interpreter, could do the heavy lifting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import numba\n",
    "\n",
    "str_to_num = {'A': 1, 'T': 2, 'G': 3, 'C': 4}\n",
    "num_to_str = {1: 'A', 2: 'T', 3: 'G', 4: 'C'}\n",
    "\n",
    "def _str_to_num(barcode):\n",
    "    converted_list = [str_to_num[b] for b in barcode]\n",
    "    return np.asarray(converted_list,dtype='int64')\n",
    "\n",
    "def _num_to_str(array):\n",
    "    converted_list = list(array)\n",
    "    return [num_to_str[b] for b in converted_list]\n",
    "    \n",
    "def dna_to_numeric(batch):\n",
    "    ''' convert list of barcode strings in batch to a numpy array '''\n",
    "    numeric_batch = np.zeros((len(batch),12),dtype=np.int64)\n",
    "    for i in range(len(batch)):\n",
    "        numeric_batch[i,:] = _str_to_num(batch[i])\n",
    "    return numeric_batch\n",
    "\n",
    "def numeric_to_dna(numeric_batch):\n",
    "    ''' convert numpy array of barcodes in batch to a list of strings '''\n",
    "    batch = np.apply_along_axis(_num_to_str, axis=1, arr=numeric_batch)\n",
    "    batch_list = batch.tolist()\n",
    "    return [''.join(b) for b in batch_list]\n",
    "\n",
    "def verify_mismatch_constraints_numpy(barcode, barcode_set, threshold=2):\n",
    "    ''' test if the current barcode will be at least 2 mismatches away from any \n",
    " element of the barcode set '''\n",
    "    \n",
    "    # broadcast substraction of barcode from each code in the barcode set\n",
    "    if barcode_set.shape[0] > 0:\n",
    "        mismatch_array = barcode_set - barcode\n",
    "        mismatch_counts = np.count_nonzero(mismatch_array,axis=1)\n",
    "        return mismatch_counts.min() >= threshold\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "def verify_content_constraints_numpy(barcode, low = 0.10, high = 0.90):\n",
    "    ''' Make sure the GC content of the barcode lies in [low,high] '''\n",
    "    gc_content = barcode[np.logical_or(barcode == 3, barcode == 4)].shape[0] / (1.0 * 12)\n",
    "    return (low < gc_content) & (gc_content < high)\n",
    "\n",
    "@numba.autojit    \n",
    "def _verify_rep_constraint(barcode, run):\n",
    "    for i in barcode:\n",
    "        result = True\n",
    "        for j in range(run.shape[0]):  \n",
    "            result = result and (barcode[i+j] == run[j])\n",
    "        if result:\n",
    "            return False    # found a run, reject this barcode\n",
    "    return True             # did not find a run, accept this barcode\n",
    "\n",
    "def verify_no_rep_constraints(barcode):\n",
    "    ''' Ensure that there are no repetitive triples of any nucleotides '''\n",
    "    result = True\n",
    "    for i in range(1,5):\n",
    "        result = result & _verify_rep_constraint(barcode, np.array([i,i,i]))\n",
    "    return result\n",
    "\n",
    "def process_batch_numpy(numeric_batch, barcodes, last_index=0, max_codes=80000):\n",
    "    ''' apply validation rules to each potential barcode in the batch '''\n",
    "    np.random.shuffle(numeric_batch)\n",
    "    for barcode in numeric_batch:\n",
    "        \n",
    "        if last_index == max_codes:\n",
    "            break\n",
    "        \n",
    "        if verify_mismatch_constraints_numpy(barcode, barcodes[0:last_index,:]) and \\\n",
    "           verify_content_constraints_numpy(barcode) and \\\n",
    "           verify_no_rep_constraints(barcode):\n",
    "            barcodes[last_index,:] = barcode\n",
    "            last_index += 1\n",
    "            \n",
    "    return last_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def get_codes_numpy(batch_size, max_codes):\n",
    "    ''' grow the set of barcodes one by one, ensuring they are 3 mismatches apart '''\n",
    "    \n",
    "    # bookkeeping for code array\n",
    "    last_index = 0\n",
    "    batch = []\n",
    "    codes = np.empty((max_codes,12),dtype=np.int64)\n",
    "    \n",
    "    for barcode in my_codes:\n",
    "        if last_index >= max_codes:\n",
    "            break\n",
    "            \n",
    "        batch.append(barcode)\n",
    "        if len(batch) == batch_size:\n",
    "            print(\"beginning with \", last_index, \" barcodes...\")\n",
    "            print(\"read \", len(batch), \" lines\")\n",
    "\n",
    "            # convert to numpy\n",
    "            batch_array = dna_to_numeric(batch)\n",
    "            t0 = time.time()\n",
    "            last_index = process_batch_numpy(batch_array, codes, last_index, max_codes)\n",
    "            t1 = time.time()\n",
    "            delta_t = t1 - t0\n",
    "            batch = []\n",
    "            print(\"result of batch processing is \", last_index, \" qualified codes\")\n",
    "            print(\"time to process was: \", delta_t)\n",
    "\n",
    "        \n",
    "\n",
    "    str_codes = numeric_to_dna(codes[0:last_index,:])\n",
    "    return str_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beginning with  0  barcodes...\n",
      "read  10000  lines\n",
      "result of batch processing is  5000  qualified codes\n",
      "time to process was:  1.1384248733520508\n",
      "beginning with  0  barcodes...\n",
      "read  10000  lines\n",
      "result of batch processing is  5000  qualified codes\n",
      "time to process was:  1.0833053588867188\n",
      "beginning with  0  barcodes...\n",
      "read  10000  lines\n",
      "result of batch processing is  5000  qualified codes\n",
      "time to process was:  1.063485860824585\n",
      "beginning with  0  barcodes...\n",
      "read  10000  lines\n",
      "result of batch processing is  5000  qualified codes\n",
      "time to process was:  1.070199966430664\n",
      "beginning with  0  barcodes...\n",
      "read  10000  lines\n",
      "result of batch processing is  5000  qualified codes\n",
      "time to process was:  1.1409730911254883\n",
      "beginning with  0  barcodes...\n",
      "read  10000  lines\n",
      "result of batch processing is  5000  qualified codes\n",
      "time to process was:  1.7324938774108887\n",
      "beginning with  0  barcodes...\n",
      "read  10000  lines\n",
      "result of batch processing is  5000  qualified codes\n",
      "time to process was:  1.1075289249420166\n",
      "beginning with  0  barcodes...\n",
      "read  10000  lines\n",
      "result of batch processing is  5000  qualified codes\n",
      "time to process was:  1.0884857177734375\n",
      "1.32 s ± 223 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 10000\n",
    "max_codes = 5000\n",
    "%timeit get_codes_numpy(batch_size, max_codes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes about the (mostly) Numpy solution\n",
    "\n",
    "- The computation time is estimated at 39.1 s ± 2.47 s for the Python solution versus 1.32 s ± 223 ms per loop for the Numpy solution, for a speedup of about 30X.  Pretty decent!  This may not be a precise estimate, since the cost of  `verify_mismatch_constraints` is $O(N^2)$ with $N$ as the intended size of the set of barcodes.  By the end of my search, I was spending much more time validating codes than the few seconds here.  This also assumes you can load all your strings into memory before processing.\n",
    "\n",
    "\n",
    "- The expensive operation is still computing the Hamming distance of each potential new barcode to all existing barcodes.  But where in the Python implementation this happens in two Python loops via list comprehensions, now it is pushed down into compiled Numpy code via broadcasting and a `nonzero_counts` + `.min()` reduction.  \n",
    "\n",
    "\n",
    "- The constraints on GC content are mostly the same, but the constraint on finding and rejecting barcodes with runs of 3 monomers is actually a bit of a hack using Numba.  Maybe there is a snappy way to express this in Numpy, but I had a hard time finding a way (though I didn't experiment with any convolution operations in scipy).  Especially since the size of the domain is so small.  So, I got to try out another fun technology to accelerate Python code.  The downside to Numba is that to get the full benefit of generated LLVM byte-code to replace your Python code, you need to be *very* careful about how you express computations.  The vectorization inherent to Numpy code is not very well supported yet, and neither are some of the syntactic sugary ways that Python & Numpy support iterator creation.  This version here is maybe my third or fourth attempt, each subsequent attempt stripped out more Pythonic idioms that the Numba JIT compiler does not support (yet).  Getting to the final iteration of `for j in range(run.shape[0]):` felt ... *odd* to have in Python code.  But Numba is in active development, and is getting better all the time.\n",
    "\n",
    "\n",
    "- A way to improve this further would be to double down on Numpy in `verify_mismatch_constraints_numpy`.  Instead of comparing potential barcodes against the set of existing codes one by one, we could sample a set of possible strings, compute their Hamming distance at a matrix, then select those which are further than a given threshold away from all other strings in the set.  This would also provide an easy way to validate the final set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to grab this code, or you can just clone the repo I made for it [here](https://github.com/lzamparo/DNAbarcodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
